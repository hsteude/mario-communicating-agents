{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from src.model.lit_module import LitModule\n",
    "from src.data.dataset import VideoLabelDataset\n",
    "import src.constants as const\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data.dataset import (VideoLabelDataset,\n",
    "                              VideoFolderPathToTensor,\n",
    "                              VideoResize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './lightning_logs/version_26/checkpoints/epoch=99-step=3199.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitModule.load_from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoLabelDataset(\n",
    "            const.LABELS_TABLE_QA_PATH,\n",
    "            img_transform=torchvision.transforms.Compose([\n",
    "                VideoFolderPathToTensor(),\n",
    "                VideoResize(const.IMG_SIZE)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=10, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos, questions, answers, hidden_states  = iter(dataloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[543.4559, 996.8821,  37.2706],\n",
       "        [543.4633, 996.8958,  37.2711],\n",
       "        [543.4277, 996.8304,  37.2686],\n",
       "        [543.3871, 996.7563,  37.2658],\n",
       "        [543.3900, 996.7614,  37.2661],\n",
       "        [543.3262, 996.6446,  37.2617],\n",
       "        [543.4123, 996.8020,  37.2676],\n",
       "        [543.3502, 996.6880,  37.2633],\n",
       "        [543.4186, 996.8138,  37.2680],\n",
       "        [543.4409, 996.8543,  37.2695]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.eval()(videos)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 772.6472,  912.8349,   36.0000],\n",
       "        [ 627.9586, 1077.1292,   25.0000],\n",
       "        [ 472.4544, 1046.5730,   47.0000],\n",
       "        [ 430.9703, 1006.6889,   30.0000],\n",
       "        [ 516.7844, 1069.5571,   38.0000],\n",
       "        [ 437.3075,  983.9614,   44.0000],\n",
       "        [ 711.1207, 1024.2825,   37.0000],\n",
       "        [ 389.1723,  911.7273,   48.0000],\n",
       "        [ 618.8253, 1055.9926,   39.0000],\n",
       "        [ 560.6606,  916.3023,   40.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(73321.6484, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss = torch.nn.MSELoss(reduction='sum')\n",
    "mse_hidden = mse_loss(predictions[0:2,:].type(torch.float32),\n",
    "                      hidden_states[0:2,:].type(torch.float32))\n",
    "mse_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73323.68388475002"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((772.6472 - 543.4424)**2 + (912.8349-996.8575)**2 + (36.0000-37.2697)**2 + \\\n",
    "(627.9586-543.4633)**2 + (1077.1292-996.8958)**2 + (25.0000-37.2711)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[543.4559, 996.8821,  37.2706],\n",
       "        [543.4633, 996.8958,  37.2711]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
