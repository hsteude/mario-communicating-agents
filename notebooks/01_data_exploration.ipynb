{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from src.model.lit_module import LitModule\n",
    "from src.data.dataset import VideoLabelDataset\n",
    "import src.constants as const\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data.dataset import (VideoLabelDataset,\n",
    "                              VideoFolderPathToTensor,\n",
    "                              VideoResize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 131144\n",
      "-rw-rw-r-- 1 ubuntu 134289692 Jan  3 20:34 'epoch=95-step=767.ckpt'\n"
     ]
    }
   ],
   "source": [
    "ll lightning_logs/version_2/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './lightning_logs/version_2/checkpoints/epoch=95-step=767.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitModule.load_from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoLabelDataset(\n",
    "            const.LABELS_TABLE_QA_PATH,\n",
    "            img_transform=torchvision.transforms.Compose([\n",
    "                VideoFolderPathToTensor(),\n",
    "                VideoResize(const.IMG_SIZE)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=10, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos, questions, answers, hidden_states, _  = iter(dataloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8450, 0.5448, 0.9398],\n",
       "        [0.1780, 0.5085, 0.1194],\n",
       "        [0.2152, 0.5859, 0.5385],\n",
       "        [0.7205, 0.5345, 0.5399],\n",
       "        [0.0955, 0.3855, 0.8686],\n",
       "        [0.1475, 0.6312, 0.9583],\n",
       "        [0.8030, 0.5446, 0.8962],\n",
       "        [0.5937, 0.5861, 0.6022],\n",
       "        [0.6369, 0.5447, 0.6521],\n",
       "        [0.4800, 0.4428, 0.6196]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.eval()(videos)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8956, 0.3618, 0.8750],\n",
       "        [0.2068, 0.9950, 0.1250],\n",
       "        [0.2169, 0.6080, 0.5417],\n",
       "        [0.6988, 0.5578, 0.5000],\n",
       "        [0.0482, 0.1709, 0.9167],\n",
       "        [0.0823, 0.5678, 0.9583],\n",
       "        [0.8293, 0.9246, 0.8750],\n",
       "        [0.5823, 0.0050, 0.5000],\n",
       "        [0.6506, 0.9899, 0.5417],\n",
       "        [0.4317, 0.1307, 0.5833]], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2628, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss = torch.nn.MSELoss(reduction='sum')\n",
    "mse_hidden = mse_loss(predictions[0:2,:].type(torch.float32),\n",
    "                      hidden_states[0:2,:].type(torch.float32))\n",
    "mse_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73323.68388475002"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((772.6472 - 543.4424)**2 + (912.8349-996.8575)**2 + (36.0000-37.2697)**2 + \\\n",
    "(627.9586-543.4633)**2 + (1077.1292-996.8958)**2 + (25.0000-37.2711)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[543.4559, 996.8821,  37.2706],\n",
       "        [543.4633, 996.8958,  37.2711]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
