{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from src.model.lit_module import LitModule\n",
    "from src.data.dataset import VideoLabelDataset\n",
    "import src.constants as const\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data.dataset import (VideoLabelDataset,\n",
    "                              VideoFolderPathToTensor,\n",
    "                              VideoResize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 131500\n",
      "-rw-rw-r-- 1 ubuntu 134652580 Jan  4 06:53 'epoch=25-step=207.ckpt'\n"
     ]
    }
   ],
   "source": [
    "ll lightning_logs/version_6/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './lightning_logs/version_6/checkpoints/epoch=25-step=207.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitModule.load_from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoLabelDataset(\n",
    "            const.LABELS_TABLE_QA_PATH,\n",
    "            img_transform=torchvision.transforms.Compose([\n",
    "                VideoFolderPathToTensor(),\n",
    "                VideoResize(const.IMG_SIZE)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=10, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos, questions, answers, hidden_states, _  = iter(dataloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2236, 0.4662, 0.9885],\n",
       "        [0.2244, 0.4721, 0.2638],\n",
       "        [0.0537, 0.4881, 0.6125],\n",
       "        [0.1221, 0.4824, 0.3305],\n",
       "        [0.2819, 0.4582, 0.0720],\n",
       "        [0.9013, 0.5235, 0.1032],\n",
       "        [0.1281, 0.4774, 0.0551],\n",
       "        [0.8868, 0.5278, 0.6848],\n",
       "        [0.4278, 0.5030, 0.6455],\n",
       "        [0.1881, 0.4996, 0.4343]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.eval()(videos)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1563, 0.1859, 1.0000],\n",
       "        [0.2144, 0.1457, 0.2500],\n",
       "        [0.0180, 0.1307, 0.6667],\n",
       "        [0.1042, 1.0000, 0.2917],\n",
       "        [0.2625, 0.3970, 0.1250],\n",
       "        [0.7976, 0.4774, 0.0417],\n",
       "        [0.0381, 0.3769, 0.0833],\n",
       "        [0.6894, 0.0955, 0.7500],\n",
       "        [0.3908, 0.2060, 0.7083],\n",
       "        [0.1503, 0.3668, 0.4167]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2628, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss = torch.nn.MSELoss(reduction='sum')\n",
    "mse_hidden = mse_loss(predictions[0:2,:].type(torch.float32),\n",
    "                      hidden_states[0:2,:].type(torch.float32))\n",
    "mse_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73323.68388475002"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((772.6472 - 543.4424)**2 + (912.8349-996.8575)**2 + (36.0000-37.2697)**2 + \\\n",
    "(627.9586-543.4633)**2 + (1077.1292-996.8958)**2 + (25.0000-37.2711)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[543.4559, 996.8821,  37.2706],\n",
       "        [543.4633, 996.8958,  37.2711]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
